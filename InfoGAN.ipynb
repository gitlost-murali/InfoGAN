{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, I'll be describing the way [Info_GANs](https://arxiv.org/pdf/1606.03657) work.\n",
    "\n",
    "For a better understanding of InfoGANs, it's better to have grip on GANs, CGANs (Conditional GANs).\n",
    "\n",
    "GAN = Generative Adversarial Network , has two neural networks , one called as generator and other is a discriminator. The \n",
    "task of generator is to mimic the probability distribution of given dataset. At a high level, a generative model means you have mapped the probability distribution of the data itself. In the case of images, that means you have a probability for every possible combination of pixel values. This also means you can generate new data points by sampling from this distribution ( by choosing combinations with large probability). In Computer vision, this means that we can generate new images entirely from no prior data.\n",
    "\n",
    "The way it works is similar to a thief and police story. Imagine that a thief always wants to generate fake notes (mimic actual images distribution / mimic actual images (pixel combinations) ) and fool the police to get away with it. Police, on the other hand, wants to determine ways to detect fake notes (To detect a sample that comes from generated probability distribution). It is like a constant mutual development process.\n",
    "\n",
    "Our stable state is having an equally trained Discriminator (Police to catch fake notes) and Generator (Skilled criminal to mimic currency).\n",
    "\n",
    "How do we do that ?\n",
    "\n",
    "In every iteration, \n",
    "\n",
    "1. Generator takes a random noise data (vector) and outputs an image (which intially looks like noise). Now you have a bunch of noisy images and true images. You pass this bunch of Noisy images (Label False) and True images (Labelled True) through Discriminator (a neural network). You could see that this turned out to be a simple supervised task of classification. So we train our Discriminator keeping generator untrained.\n",
    "\n",
    "2. We pass random noise again through Generator to generate fake images. We pass this fake images labelled as True ,(To fool the discriminator) through the discriminator. When passed through Discriminator (Police), we will know the places where we fail to fool the police. Notice the differences and we(Criminal/Generator) work on them. Note that we work on Police's current state of mind, which  means we keep discriminator untrianed during this process.\n",
    "\n",
    "know the places where we fail to fool the police == Compute loss\n",
    "\n",
    "Notice the differences and work on them  == Compute gradients and update weights of generator\n",
    "\n",
    "\n",
    "Conditional GANs = If you've gone through the above description of GANs, you might have understood that generator generates samples from random noise(Entangled Representation). Wouldn't it be nice if we input a known vector (Disentangled representation) instead of random noise ? Let's say I want to generate handwritten images of a given number. This (Label ===> model ==> image) is the reverse of image classification (image ===> model ==> Label). We are passing in conditional information to the generator for producing images. On the other hand, instead of making the discriminator just classify the images real/fake we pass the label along with the image. Now discriminating, it is classifying whether the label given to the input image is true/not.\n",
    "\n",
    "\n",
    "InfoGAN = This is similar to Conditional GANs, but we don't want to specify the information. Let's make neural networks do that for us ? But WHY ?\n",
    "\n",
    "Because when passing real world data like faces,images of buildings, there are a lot of hidden concepts a.k.a Latent concepts. Neural Networks are capable of capturing this latent concepts well because of their non-linear activation functions.\n",
    "\n",
    "Sounds good. How do we do that ?\n",
    "\n",
    "For the generator we pass in Z (Random Noise) along with C (categorical distribution). Note that C's distribution may vary with the application to Gaussian / whatever. If it is a categorical distribution, you expect C to be one-hot encoded vector.\n",
    "Like [0 0 0 1] for representing a concept. We start with a uniformly distributed C [0.2 0.2 0.2 0.2 0.2].\n",
    "\n",
    "\n",
    "And with the training process, we expect neural network architecture to update C.\n",
    "\n",
    "There's only one change compared to GANs & CGANs i.e an additional neural network Q_C to monitor C.\n",
    "\n",
    "Process is similar,\n",
    "1.Train discriminator (As usual)\n",
    "2.Train Generator (As usual)\n",
    "3.Generate samples from given random noise and initial C. These generated samples are given to Q_C, which in turn learns latent concepts from C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download mnist dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b9e1941a3cec>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)\n",
    "mb_size = 32  # batch_size\n",
    "Z_dim = 16  # Random noise input for generator\n",
    "X_dim = mnist.train.images.shape[1]\n",
    "y_dim = mnist.train.labels.shape[1]\n",
    "h_dim = 128   # Hidden layer dimension\n",
    "cnt = 0\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Articles related to Xavier initialization explaining how it deals with the saturation of inputs in activation functions.\n",
    "\n",
    "[1.Shi Yan, Medium](https://medium.com/@shiyan/xavier-initialization-and-batch-normalization-my-understanding-b5b91268c25c)\n",
    "\n",
    "[2.Andy Jones, Tumblr](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)\n",
    "\n",
    "The idea is randomizing the initial weights, so that the inputs of each activation function fall within the sweet range of the activation function. Ideally, none of the neurons should start with a trapped situation.\n",
    "\n",
    "This post is a great material on Xavier initialization. Basically it tries to make sure the distribution of the inputs to each activation function is zero mean and unit variance. To do this, it assumes that the input data has been normalized to the same distribution. The more number of inputs a neuron has, the smaller the initial weights should be, in order to compensate the number of inputs. In a word, the Xavier initialization method tries to initialize weights with a smarter value, such that neurons wonâ€™t start training in saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ##  ==================== GENERATOR ========================\n",
    "\n",
    "Input is Z (Random Noise), C (categorical distribution for 10 digits)\n",
    "\n",
    "Output is image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim + 10, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "#Initialising weights and biases for generator neural network\n",
    "\n",
    "#  (Input, Output)\n",
    " \n",
    "#  Layer 1                Layer 2     \n",
    "#  (16+10, h_dim)   ==> (h_dim, X_dim)\n",
    "\n",
    "def G(z, c):\n",
    "    inputs = torch.cat([z, c], 1)\n",
    "    h = nn.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
    "    X = nn.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ==================== DISCRIMINATOR ========================\n",
    "\n",
    "Input is the image \n",
    "\n",
    "Output is the prediction of real/fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Why = xavier_init(size=[h_dim, 1])\n",
    "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "\n",
    "def D(X):\n",
    "    h = nn.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    y = nn.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================== Q(c|X) ========================== \n",
    "\n",
    "Q (Neural Network that learns C distribution) learns from the learnt Generator from discriminator in every iteration.\n",
    "\n",
    "Input is the generated image and output is updated C, based on the generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Wqxh = xavier_init(size=[X_dim, h_dim])\n",
    "bqxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whc = xavier_init(size=[h_dim, 10])\n",
    "bhc = Variable(torch.zeros(10), requires_grad=True)\n",
    "\n",
    "\n",
    "def Q(X):\n",
    "    h = nn.relu(X @ Wqxh + bqxh.repeat(X.size(0), 1))\n",
    "    c = nn.softmax(h @ Whc + bhc.repeat(h.size(0), 1))\n",
    "    return c\n",
    "\n",
    "\n",
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "Q_params = [Wqxh, bqxh, Whc, bhc]\n",
    "params = G_params + D_params + Q_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; D_loss: 1.4468754529953003; G_loss: 2.042823314666748; Idx: 8\n",
      "Iter-1000; D_loss: 0.15183107554912567; G_loss: 4.062690258026123; Idx: 8\n",
      "Iter-2000; D_loss: 0.12419113516807556; G_loss: 3.6387805938720703; Idx: 5\n",
      "Iter-3000; D_loss: 0.15582631528377533; G_loss: 4.37735652923584; Idx: 6\n",
      "Iter-4000; D_loss: 0.14815331995487213; G_loss: 3.676138401031494; Idx: 3\n",
      "Iter-5000; D_loss: 0.508362352848053; G_loss: 4.521950721740723; Idx: 9\n",
      "Iter-6000; D_loss: 0.5160889029502869; G_loss: 2.9797861576080322; Idx: 5\n",
      "Iter-7000; D_loss: 0.4400549530982971; G_loss: 2.8511452674865723; Idx: 7\n",
      "Iter-8000; D_loss: 0.8536098003387451; G_loss: 2.386115312576294; Idx: 5\n",
      "Iter-9000; D_loss: 1.0276927947998047; G_loss: 2.3005568981170654; Idx: 8\n",
      "Iter-10000; D_loss: 0.6924546360969543; G_loss: 1.9490838050842285; Idx: 2\n",
      "Iter-11000; D_loss: 0.5206872224807739; G_loss: 2.314621925354004; Idx: 0\n",
      "Iter-12000; D_loss: 0.5898966789245605; G_loss: 2.147003173828125; Idx: 2\n",
      "Iter-13000; D_loss: 0.6004413962364197; G_loss: 2.390442371368408; Idx: 0\n",
      "Iter-14000; D_loss: 0.6914935111999512; G_loss: 2.0740694999694824; Idx: 1\n",
      "Iter-15000; D_loss: 0.8130288124084473; G_loss: 1.94557523727417; Idx: 6\n",
      "Iter-16000; D_loss: 0.6940081119537354; G_loss: 2.0182018280029297; Idx: 7\n",
      "Iter-17000; D_loss: 0.7969475984573364; G_loss: 1.9446401596069336; Idx: 1\n",
      "Iter-18000; D_loss: 0.9784741401672363; G_loss: 1.5049597024917603; Idx: 2\n",
      "Iter-19000; D_loss: 0.8315763473510742; G_loss: 2.353334665298462; Idx: 8\n",
      "Iter-20000; D_loss: 0.674652099609375; G_loss: 1.9641211032867432; Idx: 8\n",
      "Iter-21000; D_loss: 0.8623499870300293; G_loss: 1.9804936647415161; Idx: 6\n",
      "Iter-22000; D_loss: 0.8295484781265259; G_loss: 2.2931160926818848; Idx: 1\n",
      "Iter-23000; D_loss: 0.8987234234809875; G_loss: 1.3899403810501099; Idx: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-39b4d8dc1892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mD_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m## Backpropagate (Calculate gradients)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mD_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m## Update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# Housekeeping - reset gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kmura\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = Variable(data.new().resize_as_(data).zero_())\n",
    "\n",
    "\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "Q_solver = optim.Adam(G_params + Q_params, lr=1e-3)\n",
    "\n",
    "\n",
    "### Generate a categorical distribution, with equal probability for each of the ten elements.\n",
    "\n",
    "# Remember that we start with a random categorical distribution [0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ]\n",
    "\n",
    "# At the end, we expect InfoGAN to embed latent contents into categorical distribution that is , categorical representation\n",
    "\n",
    "# for each digit (Like one-hot encoding)\n",
    "\n",
    "def sample_c(size):\n",
    "    c = np.random.multinomial(1, 10*[0.1], size=size)\n",
    "    c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "    return c\n",
    "### We start with sample_c\n",
    "\n",
    "\n",
    "\n",
    "## 100000\n",
    "\n",
    "for it in range(100000):\n",
    "    \n",
    "    \n",
    "    # Sample data\n",
    "    \n",
    "    X, _ = mnist.train.next_batch(mb_size)   ## Get a batch of size 32\n",
    "    X = Variable(torch.from_numpy(X))        ## Convert to torch variable for updating it.    \n",
    "\n",
    "    z = Variable(torch.randn(mb_size, Z_dim)) ## z (32,16) ==> 16-size random vector for each example\n",
    "    c = sample_c(mb_size)                     ## Create uniform categorical distribution for the batch. (32,10) [[0.2,..0.2],[0.2,..,0.2]]  \n",
    "\n",
    "    \n",
    "\n",
    "    # Dicriminator forward-loss-backward-update (Only Train Disc but not generator and Q_c)\n",
    "\n",
    "# ================================== START ==========================================\n",
    "\n",
    "    G_sample = G(z, c)                       ## Generate images given random noise z and prior c.\n",
    "    D_real = D(X)                            ## Make predictions on the actual images.Ideally D_real shud classify it as real.\n",
    "    D_fake = D(G_sample)                     ## Make predictions on the fake (generated) images.\n",
    "\n",
    "    D_loss = -torch.mean(torch.log(D_real + 1e-8) + torch.log(1 - D_fake + 1e-8))\n",
    "    \n",
    "    ## Classify real images as True (1) and fake images as False (0)\n",
    "\n",
    "    D_loss.backward()   ## Backpropagate (Calculate gradients)\n",
    "    D_solver.step()     ## Update weights\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "# ================================= STOP =============================================\n",
    "\n",
    "\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "\n",
    "# ================================== START ==========================================\n",
    "\n",
    "\n",
    "    G_sample = G(z, c)                    ## Generate images given random noise z and prior c.\n",
    "    D_fake = D(G_sample)                  ## Make predictions on the fake (generated) images.\n",
    "\n",
    "    G_loss = -torch.mean(torch.log(D_fake + 1e-8))\n",
    "    \n",
    "    ## Classify fake images as True (1)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    \n",
    "# ================================== STOP ==========================================\n",
    "\n",
    "\n",
    "    # Q forward-loss-backward-update\n",
    "    \n",
    "# ================================== START ==========================================\n",
    "\n",
    "    G_sample = G(z, c)                    ## Generate images given random noise z and prior c.\n",
    "    Q_c_given_x = Q(G_sample)             ## Generate Q_C (New C)\n",
    "\n",
    "    crossent_loss = torch.mean(-torch.sum(c * torch.log(Q_c_given_x + 1e-8), dim=1))\n",
    "    mi_loss = crossent_loss\n",
    "\n",
    "    mi_loss.backward()\n",
    "    Q_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "# ================================== STOP ==========================================\n",
    "\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        idx = np.random.randint(0, 10)\n",
    "        c = np.zeros([mb_size, 10])\n",
    "        c[range(mb_size), idx] = 1\n",
    "        c = Variable(torch.from_numpy(c.astype('float32')))\n",
    "        samples = G(z, c).data.numpy()[:16]\n",
    "\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}; Idx: {}'\n",
    "              .format(it, D_loss.data.numpy(), G_loss.data.numpy(), idx))\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'\n",
    "                    .format(str(cnt).zfill(3)+\"label_\"+str(idx)), bbox_inches='tight')\n",
    "        cnt += 1\n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
